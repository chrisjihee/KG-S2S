(KG-S2S) dlt@dl012:~/proj/KG-S2S$ python --version
Python 3.11.6
(KG-S2S) dlt@dl012:~/proj/KG-S2S$ pip list | grep torch
pytorch-lightning   1.9.5
torch               2.1.1+cu118
torchmetrics        1.2.1
(KG-S2S) dlt@dl012:~/proj/KG-S2S$ pip list | grep trans
transformers        4.24.0
(KG-S2S) dlt@dl012:~/proj/KG-S2S$ bash setting-korean/train-FB15k-237N-ko-T5.sh
config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.21k/1.21k [00:00<00:00, 5.14MB/s]
Namespace(dataset_path='./data/processed', dataset='FB15k-237N-ko', model='T5Finetuner', gpu='6', seed=41504, num_workers=4, save_dir='./checkpoint/FB15k-237N-ko-2023-12-11 14:18:56.295543', pretrained_model='t5-base', batch_size=32, val_batch_size=8, num_beams=40, num_beam_groups=1, src_max_length=512, train_tgt_max_length=512, eval_tgt_max_length=90, epochs=10, lr=0.001, diversity_penalty=0.0, model_path='', optim='Adam', decoder='beam_search', log_text=False, use_prefix_search=False, src_descrip_max_length=240, tgt_descrip_max_length=240, use_soft_prompt=True, use_rel_prompt_emb=True, skip_n_val_epoch=0, seq_dropout=0.2, temporal=False, n_ent=12068, n_rel=90, vocab_size=32128, model_dim=768)
Global seed set to 41504
spiece.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 904kB/s]
tokenizer.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.39M/1.39M [00:01<00:00, 1.35MB/s]
/data/dlt/mambaforge/envs/KG-S2S/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
tokenizing entities...
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12068/12068 [00:06<00:00, 1867.89it/s]
/data/dlt/mambaforge/envs/KG-S2S/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.
For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.
- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.
- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.
- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.
  warnings.warn(
datamodule construction done.

----------------------------------------------------------------------------------
 * pl.Trainer(accelerator=gpu, devices=[6])
----------------------------------------------------------------------------------

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 892M/892M [00:46<00:00, 19.3MB/s]
model construction done.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name                       | Type                               | Params
----------------------------------------------------------------------------------
0 | T5ForConditionalGeneration | ModifiedT5ForConditionalGeneration | 222 M
1 | rel_embed1                 | Embedding                          | 69.1 K
2 | rel_embed2                 | Embedding                          | 69.1 K
3 | rel_embed3                 | Embedding                          | 69.1 K
4 | rel_embed4                 | Embedding                          | 69.1 K
----------------------------------------------------------------------------------
223 M     Trainable params
0         Non-trainable params
223 M     Total params
892.720   Total estimated model params size (MB)
Training: 0it [00:00, ?it/s]
Epoch:    0,
Epoch 0:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 4569/6023 [09:21<02:58,  8.14it/s, loss=0.264]
Training time:  561s
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6023/6023 [1:28:03<00:00,  1.14it/s, loss=0.264]
                   mrr          mr   hit@1   hit@3  hit@10
tail ranking  0.928044  297.703079  90.76%  94.98%  95.29%
head ranking  0.822391  772.406331  78.07%  86.60%  87.36%
mean ranking  0.875218  535.054705  84.41%  90.79%  91.32%
Ellipsis
Validation time: 4721s
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6023/6023 [1:28:03<00:00,  1.14it/s, loss=0.264]
Total time: 5283s, loss: 0.3287
--------------------------------------------------
Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6023/6023 [1:28:03<00:00,  1.14it/s, loss=0.264]
KeyboardInterrupt
(KG-S2S) dlt@dl012:~/proj/KG-S2S$

